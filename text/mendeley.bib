@article{Bouzy2007,
abstract = {Monte-Carlo Tree Search (MCTS) is a new best-ﬁrst search guided by the results of Monte-Carlo simulations. In this article we introduce two progressive strategies for MCTS, called progressive bias and progressive unpruning. They enable the use of rel- atively time-expensive heuristic knowledge without speed reduction. Progressive bias directs the search according to heuristic knowledge. Progressive unpruning ﬁrst reduces the branching factor, and then increases it gradually again. Experiments assess that the two progressive strategies signiﬁcantly improve the level of our Go program Mango. Moreover, we see that the combination of both strategies performs even better on larger board sizes.},
author = {Bouzy, Bruno},
issn = {17930057},
journal = {Computer},
number = {3},
pages = {343},
publisher = {Citeseer},
title = {{Progressive Strategies for Monte-Carlo Tree Search}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.77.9239\&amp;rep=rep1\&amp;type=pdf},
volume = {4},
year = {2007}
}
@misc{Dai2011,
abstract = {Ms. Pac-Man is a popular chasing and evading game and the ghost character in the game is controlled by script. This article evolved an evolutionary neural network for the red ghost to chase Pac-Man. Red ghost' position, Pac-Man's position and Pac-Man's state are considered to be the inputs of the neural network, and the output is the direction of Red ghost to move in the next step. We also proposed a fitness function to raise capture ability in evolution so that the Red ghost learns by itself in simulation. Experimental results show that the agent learns well and plays better in teamwork than the traditional script controlled ghost.},
author = {Dai, Jia-Yue and Li, Yan and Chen, Jun-Fen and Zhang, Feng},
booktitle = {2011 International Conference on Machine Learning and Cybernetics},
institution = {Machine Learning Center, Faculty of Mathematics and Computer Science, Hebei University, Baoding 071002, China},
isbn = {9781457703072},
issn = {2160133X},
keywords = {chasing evading game,evolutionary neural network,game ai,pac man},
pages = {732--736},
publisher = {IEEE},
title = {{Evolutionary neural network for ghost in Ms. Pac-Man}},
volume = {2},
year = {2011}
}
@article{Chaslot2008,
abstract = {Monte-Carlo Tree Search (MCTS) is a new best-first search method that started a revolution in the field of Computer Go. Paral- lelizing MCTS is an important way to increase the strength of any Go program. In this article, we discuss three parallelization methods for MCTS: leaf parallelization, root parallelization,and tree parallelization. To be effective tree parallelization requires two techniques: adequately handling of (1) local mutexes and (2) virtual loss. Experiments in 1313 Go reveal that in the program Mango root parallelization may lead to the best results for a specific time setting and specific program parame- ters. However, as soon as the selection mechanism is able to handle more adequately the balance of exploitation and exploration, tree paralleliza- tion should have attention too and could become a second choice for parallelizing MCTS. Preliminary experiments on the smaller 99 board provide promising prospects for tree parallelization.},
author = {Chaslot, Guillaume M and Winands, Mark H and {Van Den Herik}, Jaap H},
issn = {15302075},
journal = {Computers and Games},
pages = {60--71},
publisher = {Springer},
title = {{Parallel Monte-Carlo Tree Search}},
url = {http://www.springerlink.com/index/u39846x66k535218.pdf},
volume = {5131},
year = {2008}
}
@article{Kocsis2006,
abstract = {For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.},
author = {Kocsis, Levente and Szepesv\'{a}ri, Csaba},
doi = {10.1007/11871842},
editor = {F\"{u}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
isbn = {9783540453758},
issn = {03029743},
journal = {Machine Learning ECML 2006},
number = {1},
pages = {282--293},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Bandit based Monte-Carlo Planning}},
url = {http://www.springerlink.com/index/D232253353517276.pdf},
volume = {4212},
year = {2006}
}
@misc{Nguyen2011,
abstract = {We present an application of Monte-Carlo Tree Search (MCTS) to controlling ghosts in the game of Ms Pac-Man. We approach the problem by performing MCTS on each ghost's tree that represents the game state from the ghost's perspective. Our goal is to create a strong ghost team that is adaptable to a variety of Ms Pac-Man's play styles. This ghost team (ICE gUCT) won the CEC 2011 Ms Pac-Man vs Ghost Team Competition for the ghost side.},
author = {Nguyen, Kien Quang and Thawonmas, Ruck},
booktitle = {2011 IEEE International Games Innovation Conference IGIC},
doi = {10.1109/IGIC.2011.6114952},
institution = {Intelligent Computer Entertainment Laboratory, Ritsumeikan University, Japan},
isbn = {9781457702570},
pages = {8--11},
publisher = {IEEE},
title = {{Applying Monte-Carlo Tree Search to collaboratively controlling of a Ghost Team in Ms Pac-Man}},
year = {2011}
}
@misc{Stefanovitch2011,
abstract = {In this paper we address the problem of efficient decentralised coordination of cooperative multi-agent systems by taking into account the structure of the problem. We consider coordination problems that can be framed as Distributed Constraint Optimization Problems (DCOP).Graphical models such as junction trees are widely used in order to exploit the structure of an inference problem so as to minimise the complexity of the optimal solution of such problems. In this paper we propose an extension of the junction tree inference algorithm in order to provide it with properties befitting more closely multi-agent systems operational constraints. Specifically, our contribution is able to provide an adjustable trade-off between the make span of the coordination process and its decentralisation as well as the solution quality. We provide bounds, and report empirical results on two kinds of instances of coordination networks.},
author = {Stefanovitch, Nicolas and Seghrouchni, Amal El Fallah},
booktitle = {2011 IEEEWICACM International Conferences on Web Intelligence and Intelligent Agent Technology},
doi = {10.1109/WI-IAT.2011.230},
isbn = {9780769545134},
keywords = {approximation,dcop,decentralisation,dyn dcop,gdl,junction tree},
pages = {377--380},
publisher = {IEEE},
title = {{Partially Decentralised Junction Trees for Quality Guaranteed Approximate Coordination in Open Multi-agent Systems with Optimality/Runtime Trade-Off}},
volume = {2},
year = {2011}
}
@article{Teytaud2008,
abstract = {Since their impressive successes in various areas of large-scale parallelization, recent techniques like UCT and other Monte-Carlo planning variants (Kocsis and Szepesvari, 2006a) have been extensively studied (Coquelin and Munos, 2007; Wang and Gelly, 2007). We here propose and compare various forms of parallelization of bandit-based tree-search, in particular for our computer-go algorithm MoGo.},
author = {Teytaud, Olivier and Baptiste, Jean and Gelly, Sylvain and Rimmel, Arpad and Kalemkarian, Yann},
file = {::},
journal = {ICINCO},
keywords = {learning,statistics \& optimisation},
pages = {198--203},
title = {{THE PARALLELIZATION OF MONTE-CARLO PLANNING}},
url = {http://eprints.pascal-network.org/archive/00006886/},
year = {2008}
}
@article{Undeger2009,
abstract = {In this paper, we address the problem of multi-agent pursuit in dynamic and partially observable environments, modeled as grid worlds; and present an algorithm called Multi-Agent Real-Time Pursuit (MAPS) for multiple predators to capture a moving prey cooperatively. MAPS introduces two new coordination strategies namely Blocking Escape Directions and Using Alternative Proposals, which help the predators waylay the possible escape directions of the prey in coordination. We compared our coordination strategies with the uncoordinated one against a prey controlled by Prey A, and observed an impressive reduction in the number of moves to catch the prey.},
author = {Undeger, Cagatay and Polat, Faruk},
doi = {10.1007/s10458-009-9102-0},
issn = {13872532},
journal = {Autonomous Agents and MultiAgent Systems},
keywords = {multi agent search,path planning,real time pursuit,real time search},
number = {1},
pages = {69--107},
publisher = {Springer Netherlands},
title = {{Multi-agent real-time pursuit}},
url = {http://www.springerlink.com/index/10.1007/s10458-009-9102-0},
volume = {21},
year = {2009}
}
@misc{Zivan2009,
abstract = {A team of mobile sensors can be used for coverage of targets in different environments. The dynamic nature of such an application requires the team of agents to adjust their locations with respect to changes which occur. The dynamic nature is caused by environment changes, changes in the agents\&amp;x02019; tasks and by technology failures. A new model for representing problems of mobile sensor teams based on Distributed Constraint Optimization Problems (DCOP), is proposed. The proposed model, needs to handle a dynamic problem in which the alternative assignments for agents and set of neighbors, derive from their physical location which is dynamic. DCOP MST enables representation of variant dynamic elements which a team of mobile sensing agents face. A reputation model is used to determine the credibility of agents. By representing the dynamic sensing coverage requirements in the same scale as the agents\&amp;x02019; credibility, the deployment of sensors in the area can be evaluated and adjusted with correspondence to dynamic changes. In order to solve a DCOP MST, a local (incomplete) search algorithm (MGM MST) based on the MGM algorithm is proposed and combined with various exploration methods. While existing exploration methods are evidently not effective in DCOP MSTs, new exploration methods which are designed for these special applications are found to be successful in our experimental study.},
author = {Zivan, Roie and Glinton, Robin and Sycara, Katia},
booktitle = {2009 IEEEWICACM International Joint Conference on Web Intelligence and Intelligent Agent Technology},
doi = {10.1109/WI-IAT.2009.176},
isbn = {9780769538013},
pages = {347--354},
publisher = {Ieee},
title = {{Distributed Constraint Optimization for Large Teams of Mobile Sensing Agents}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5284814},
volume = {2},
year = {2009}
}
@misc{Kato2010,
abstract = {Recently Monte-Carlo tree search is boosting the performance of computer Go playing pro- grams. A novel parallel Monte-Carlo tree search algorithm is proposed. A tree searcher runs on a client computer and multiple Monte-Carlo simulation servers run on other computers on a network. The client broadcasts a position to be simulated to every server, which then simulates a game from the position to the end and sends the result (win or loss) back to the client. The statistical information in the search tree is updated by the client according to the result. This architecture allows servers on-the-ﬂy connection or disconnection. Experimental results using four quad-core Linux computers on a private Gigabit Ethernet LAN show its performance scales well.},
author = {Kato, Hideki and Takeuchi, Ikuo},
booktitle = {2010 International Conference on Technologies and Applications of Artificial Intelligence},
doi = {10.1109/TAAI.2010.83},
isbn = {9781424486687},
pages = {491--498},
publisher = {IEEE},
title = {{Parallel Monte-Carlo Tree Search with Simulation Servers}},
url = {http://www.gggo.jp/publications/gpw08-private.pdf},
year = {2010}
}
@article{Auer2002,
abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
author = {Auer, P and Cesa-Bianchi, N and Fischer, P},
doi = {10.1023/A:1013689704352},
editor = {Kivinen, Jyrki},
issn = {08856125},
journal = {Machine Learning},
keywords = {adaptive allocation rules,bandit problems,finite horizon regret},
number = {2},
pages = {235--256},
publisher = {Springer},
title = {{Finite-time Analysis of the Multiarmed Bandit Problem}},
url = {http://www.springerlink.com/index/L7V1647363415H1T.pdf},
volume = {47},
year = {2002}
}
@article{Badal2006,
abstract = {Despite the fact that fast computers are nowadays available at low cost, there are many situations where obtaining a reasonably low statistical uncertainty in a Monte Carlo (MC) simulation involves a prohibitively large amount of time. This limitation can be overcome by having recourse to parallel computing. Most tools designed to facilitate this approach require modification of the source code and the installation of additional software, which may be inconvenient for some users. We present a set of tools, named clonEasy, that implement a parallelization scheme of a MC simulation that is free from these drawbacks. In clonEasy, which is designed to run under Linux, a set of clone CPUs is governed by a master computer by taking advantage of the capabilities of the Secure Shell (ssh) protocol. Any Linux computer on the Internet that can be ssh-accessed by the user can be used as a clone. A key ingredient for the parallel calculation to be reliable is the availability of an independent string of random numbers for each CPU. Many generatorssuch as RANLUX, RANECU or the Mersenne Twistercan readily produce these strings by initializing them appropriately and, hence, they are suitable to be used with clonEasy. This work was primarily motivated by the need to find a straightforward way to parallelize PENELOPE, a code for MC simulation of radiation transport that (in its current 2005 version) employs the generator RANECU, which uses a combination of two multiplicative linear congruential generators (MLCGs). Thus, this paper is focused on this class of generators and, in particular, we briefly present an extension of RANECU that increases its period up to View the MathML source and we introduce seedsMLCG, a tool that provides the information necessary to initialize disjoint sequences of an MLCG to feed different CPUs. This program, in combination with clonEasy, allows to run PENELOPE in parallel easily, without requiring specific libraries or significant alterations of the sequential code.},
author = {Badal, A and Sempau, J},
doi = {10.1016/j.cpc.2006.05.009},
issn = {00104655},
journal = {Computer Physics Communications},
number = {6},
pages = {440--450},
publisher = {ELSEVIER SCIENCE BV},
title = {{A package of Linux scripts for the parallelization of Monte Carlo simulations}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S001046550600230X},
volume = {175},
year = {2006}
}
@misc{Cazenave2009,
abstract = {We address the parallelization of a Monte-Carlo search algorithm. On a cluster of 64 cores we obtain a speedup of 56 for the parallelization of Morpion solitaire. An algorithm that behaves better than a naive one on heterogeneous clusters is also detailed.},
author = {Cazenave, T and Jouandeau, N},
booktitle = {2009 IEEE International Symposium on Parallel Distributed Processing},
doi = {10.1109/IPDPS.2009.5161122},
isbn = {9781424437511},
issn = {15302075},
keywords = {monte carlo,morpion solitaire,parallelization,search},
pages = {1--6},
publisher = {IEEE},
title = {{Parallel Nested Monte-Carlo search}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5161122},
volume = {0},
year = {2009}
}
@inproceedings{Cazenave2007,
abstract = {Abstract. We present three parallel algorithms for UCT . For 99 Go, they all improve the results of the programs that use them against GNU GO 3.6. The simplest one, the single-run algorithm, uses very few communications and shows improvements comparable to the ...},
author = {Cazenave, Tristan and Jouandeau, Nicolas},
booktitle = {Proceedings of the Computer Games},
pages = {93--101},
publisher = {Citeseer},
title = {{On the Parallelization of UCT}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.77.5065\&amp;rep=rep1\&amp;type=pdf},
year = {2007}
}
@article{Coquelin2007,
abstract = {Bandit based methods for tree search have recently gained popularity when applied to huge trees, e.g. in the game of go (Gelly et al., 2006). The UCT algorithm (Kocsis and Szepesvari, 2006), a tree search method based on Upper Confidence Bounds (UCB) (Auer et al., 2002), is believed to adapt locally to the effective smoothness of the tree. However, we show that UCT is too ``optimistic'' in some cases, leading to a regret O(exp(exp(D))) where D is the depth of the tree. We propose alternative bandit algorithms for tree search. First, a modification of UCT using a confidence sequence that scales exponentially with the horizon depth is proven to have a regret O(2 D sqrtn), but does not adapt to possible smoothness in the tree. We then analyze Flat-UCB performed on the leaves and provide a finite regret bound with high probability. Then, we introduce a UCB-based Bandit Algorithm for Smooth Trees which takes into account actual smoothness of the rewards for performing efficient ``cuts'' of sub-optimal branches with high confidence. Finally, we present an incremental tree search version which applies when the full tree is too big (possibly infinite) to be entirely represented and show that with high probability, essentially only the optimal branches is indefinitely developed. We illustrate these methods on a global optimization problem of a Lipschitz function, given noisy data.},
author = {Coquelin, Pierre-Arnaud and Munos, R\'{e}mi},
file = {::},
institution = {INRIA},
journal = {Arxiv preprint cs0703062},
number = {March},
pages = {67--74},
publisher = {AUAI Press},
title = {{Bandit Algorithms for Tree Search}},
url = {http://arxiv.org/abs/cs/0703062},
volume = {23},
year = {2007}
}
@article{Coulom2006,
abstract = {A Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations. The method can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a Monte-Carlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to min-max as the number of simulations grows. This approach provides a fine-grained control of the tree growth, at the level of individual simulations, and allows efficient selectivity. The resulting algorithm was implemented in a 9x9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.},
author = {Coulom, R\'{e}mi},
doi = {10.1007/978-3-540-75538-8\_7},
isbn = {3540755373},
journal = {Search},
pages = {72--83},
publisher = {Springer-Verlag},
series = {Lecture Notes in Computer Science},
title = {{Efficient selectivity and backup operators in Monte-Carlo tree search}},
url = {http://portal.acm.org/citation.cfm?id=1777826.1777833},
volume = {4630},
year = {2006}
}
@inproceedings{Hoock2010,
abstract = {We consider the validation of randomly generated patterns in a Monte-Carlo Tree Search program. Our bandit-based genetic programming (BGP) algorithm, with proved mathematical properties, outperformed a highly optimized handcrafted module of a well-known computer-Go program with several world records in the game of Go.},
author = {Hoock, Jean-Baptiste and Teytaud, Olivier},
booktitle = {Proceedings of the 13th European Conference on Genetic Programming EuroGP 2010},
doi = {10.1007/978-3-642-12148-7\_23},
editor = {Esparcia-Alcazar, Anna Isabel and Ekart, Aniko and Silva, Sara and Dignum, Stephen and Uyar, A Sima},
pages = {268--277},
publisher = {Springer},
series = {LNCS},
title = {{Bandit-Based Genetic Programming}},
url = {http://www.springerlink.com/index/f24077w640563877.pdf},
volume = {6021},
year = {2010}
}
@article{Chaslot2006,
abstract = {The game of Go is one of the games that still withstand classical Artificial Intelligence approaches. Hence, it is a good testbed for new AI methods. Amongst them, Monte-Carlo led to promising results. This method consists of building an evaluation function by averaging the outcome of several randomized games. The paper introduces a new strategy, which we call Objective Monte-Carlo, to improve this evaluation. Objective Monte-Carlo is composed of two parts. The first one is a move-selection strategy that adjusts the amount of exploration and exploitation automatically. We show experimentally that it outperforms the two classi- cal strategies previously proposed for Monte-Carlo Go: Simulated Annealing and Progressive Pruning. The second part of our algorithm is a new backpropagation strategy. We show that it gives better results than Minimax in this context. Finally we discuss the extension of this method to other problems.},
author = {Chaslot, Guillaume Maurice Jean-Bernard and Saito, Jahn-Takeshi and Uiterwijk, Jos W H M and Bouzy, Bruno and {Van Den Herik}, H Jaap},
editor = {Schobbens, P Y and Vanhoof, W and Schwanen, G},
journal = {Proceedings of the 18th BeNeLux Conference on Artificial Intelligence},
pages = {83--90},
title = {{Monte-Carlo Strategies for Computer Go}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.97.8924\&amp;rep=rep1\&amp;type=pdf},
year = {2006}
}
@article{Mahajan2008,
abstract = {Multi-armed bandit (MAB) problems are a class of sequential resource allo- cation problems concerned with allocating one or more resources among several alternative (competing) projects. Such problems are paradigms of a fun- damental conflict between making decisions (allocating resources) that yield high current rewards, versus making decisions that sacrifice current gains with the prospect of better future rewards. The MAB formulation models resource allocation problems arising in several technological and scientific disciplines such as sensor management, manufacturing systems, economics, queueing and communication networks, clinical trials, control theory, search theory, etc. (see 88 and references therein).},
author = {Mahajan, Aditya},
chapter = {6},
doi = {10.1007/978-0-387-49819-5\_6},
journal = {Foundations and Applications of Sensor Management},
pages = {121-- 151},
publisher = {Springer US},
title = {{Chapter 6 MULTI-ARMED BANDIT PROBLEMS}},
url = {http://www.springerlink.com/index/RX4L35L04K022G37.pdf},
year = {2008}
}
@article{Shafiei2009,
abstract = {Simultaneous move games where all the player have to take their actions simultaneously are a class of games in general game playing. In this paper we analyze how UCT performs in this class of games. We argue that UCT does not converge to a Nash equilibrium in general and the situation that it con- verges to can be exploited. We also analyze CFR (CounterFactual Regret) and show how it can be used to exploit UCT.},
author = {Shafiei, Mohammad and Sturtevant, Nathan and Schaeffer, Jonathan},
journal = {Citeseer},
publisher = {Citeseer},
title = {{Comparing UCT versus CFR in Simultaneous Games}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.161.6138},
year = {2009}
}
